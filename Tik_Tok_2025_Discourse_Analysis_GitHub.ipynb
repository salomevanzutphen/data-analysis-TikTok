{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**1.0 Data Preparation (non-rerunnable, privacy-restriced)**"
      ],
      "metadata": {
        "id": "cYz1qiyOaWcR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The comments have been extracted and turned into a dataset, accessible via google drive. This part reviews the data and prepares it by removing any deleted comments, adding a stable index and anonymising usernames for privacy reasons. The stable index ensures consistency during the thematic coding, especially considering the dataset may be reloaded which causes default indexes to change.\n",
        "\n",
        "For privacy reasons, this part of the data analysis is not rerunable in Github, as the file contains the original authornames. For rerunning the code, please start from step **1.1 GitHub-Ready Code**."
      ],
      "metadata": {
        "id": "hlaSxCC_VCG_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "D0FLebWGaKmI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "file_path = \"/content/drive/MyDrive/Dataprojects/Methodology/dataset_tiktok-comments-scraper_2025-11-04_02-52-04-694.csv\"\n",
        "df_new = pd.read_csv(file_path)\n",
        "\n",
        "print(f\"âœ… Loaded {len(df_new):,} rows from {file_path}\")\n",
        "pd.set_option('display.max_colwidth', None)\n",
        "df_new['text'].head()"
      ],
      "metadata": {
        "id": "UtMfUfdxlMt4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop NaN in 'text'\n",
        "rows_before_drop = len(df_new)\n",
        "df_new = df_new.dropna(subset=['text']).copy()\n",
        "rows_after_drop = len(df_new)\n",
        "\n",
        "print(f\"âœ… Initial rows: {rows_before_drop:,}\")\n",
        "print(f\"ðŸ—‘ï¸ Rows removed due to NaN in 'text': {rows_before_drop - rows_after_drop:,}\")\n",
        "print(f\"âœ¨ Rows remaining: {rows_after_drop:,}\")\n",
        "\n",
        "if \"text\" in df_new.columns and \"comment\" not in df_new.columns:\n",
        "    df_new.rename(columns={\"text\": \"comment\"}, inplace=True)\n",
        "    print(\"ðŸ“ Renamed 'text' to 'comment'.\")\n",
        "\n",
        "if \"diggCount\" in df_new.columns and \"likes\" not in df_new.columns:\n",
        "    df_new.rename(columns={\"diggCount\": \"likes\"}, inplace=True)\n",
        "    print(\"ðŸ“ Renamed 'diggCount' to 'likes'.\")\n",
        "\n",
        "df_new[\"comment\"] = df_new[\"comment\"].astype(str)\n",
        "df_new = df_new[df_new[\"comment\"].str.strip().str.len() > 0].copy()\n",
        "\n",
        "# Create anonymized user column\n",
        "if \"uid\" in df_new.columns and \"anon_user\" not in df_new.columns:\n",
        "    unique_uids = df_new[\"uid\"].dropna().unique()\n",
        "    uid_to_anon = {uid: f\"User_{i+1}\" for i, uid in enumerate(unique_uids)}\n",
        "    df_new[\"anon_user\"] = df_new[\"uid\"].map(uid_to_anon)\n",
        "    print(f\"ðŸ‘¤ Created 'anon_user' for {len(unique_uids):,} users.\")\n",
        "\n",
        "# âœ… Privacy: drop identifying columns AFTER anon_user is created\n",
        "cols_to_drop = [c for c in [\"author\", \"uid\"] if c in df_new.columns]\n",
        "if cols_to_drop:\n",
        "    df_new.drop(columns=cols_to_drop, inplace=True)\n",
        "    print(f\"ðŸ”’ Dropped identifying columns for privacy: {cols_to_drop}\")\n",
        "\n",
        "# add stable index\n",
        "df_new = df_new.reset_index(drop=True)\n",
        "\n",
        "if \"stable_index\" not in df_new.columns:\n",
        "    df_new[\"stable_index\"] = df_new.index\n",
        "    print(\"ðŸ“Œ Added stable 'stable_index' column.\")\n",
        "\n",
        "print(\"\\nFinal TikTok cleaned dataframe info:\")\n",
        "df_new.info()\n",
        "\n",
        "clean_tiktok_path = \"/content/drive/MyDrive/Dataprojects/Methodology/tiktok_cleaned_comments.csv\"\n",
        "df_new.to_csv(clean_tiktok_path, index=False, encoding=\"utf-8\")\n",
        "print(f\"\\nðŸ’¾ Cleaned TikTok dataset saved to:\\n{clean_tiktok_path}\")\n"
      ],
      "metadata": {
        "id": "_hVUyjK1VHbq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1.1 GitHub-Ready Code**"
      ],
      "metadata": {
        "id": "lR-uhnAbN57i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Fresh clone (no cache) ---\n",
        "import shutil\n",
        "from pathlib import Path\n",
        "import os\n",
        "\n",
        "REPO = \"data-analysis-TikTok\"\n",
        "REPO_URL = \"https://github.com/salomevanzutphen/data-analysis-TikTok.git\"\n",
        "\n",
        "if Path(REPO).exists():\n",
        "    shutil.rmtree(REPO)\n",
        "\n",
        "!git clone {REPO_URL}\n",
        "os.chdir(REPO)\n",
        "\n",
        "!ls -lh data/dataset/tiktok_cleaned_comments.csv\n"
      ],
      "metadata": {
        "id": "bUgxSYRLOnEe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df_tiktok = pd.read_csv(\"data/dataset/tiktok_cleaned_comments.csv\")\n",
        "print(df_tiktok.shape)\n",
        "df_tiktok.head()\n"
      ],
      "metadata": {
        "id": "1xF7UYXEOqAf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "\n",
        "CANDIDATE_PATHS = [\n",
        "    Path(\"data/dataset/tiktok_cleaned_comments.csv\"),\n",
        "    Path(\"../data/dataset/tiktok_cleaned_comments.csv\"),\n",
        "]\n",
        "\n",
        "CLEAN_TIKTOK_DATA = next((p for p in CANDIDATE_PATHS if p.exists()), None)\n",
        "if CLEAN_TIKTOK_DATA is None:\n",
        "    raise FileNotFoundError(\n",
        "        \"Could not find tiktok_cleaned_comments.csv.\\n\"\n",
        "        \"Expected it at data/dataset/tiktok_cleaned_comments.csv\"\n",
        "    )\n",
        "\n",
        "df_tiktok = pd.read_csv(CLEAN_TIKTOK_DATA)\n",
        "print(f\"âœ… Loaded {len(df_tiktok):,} TikTok rows from: {CLEAN_TIKTOK_DATA}\")\n"
      ],
      "metadata": {
        "id": "LN87g-wiOs3f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1.2 Reviewing Duplicate Comments**"
      ],
      "metadata": {
        "id": "KUFh8H7m1P54"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import pandas as pd\n",
        "from IPython.display import display\n",
        "\n",
        "df_tiktok = pd.read_csv(CLEAN_TIKTOK_DATA)\n",
        "print(f\"Loaded {len(df_tiktok):,} TikTok rows\")\n",
        "\n",
        "text_col = \"comment\"\n",
        "\n",
        "df_tiktok[\"createTimeISO\"] = pd.to_datetime(df_tiktok[\"createTimeISO\"], errors=\"coerce\")\n",
        "\n",
        "duplicate_counts = df_tiktok[text_col].value_counts()\n",
        "duplicate_texts = duplicate_counts[duplicate_counts > 1].index.tolist()\n",
        "\n",
        "print(f\"ðŸ” Found {len(duplicate_texts)} duplicated unique TikTok comments.\")\n",
        "\n",
        "duplicates_full = df_tiktok[df_tiktok[text_col].isin(duplicate_texts)].copy()\n",
        "\n",
        "def summarize_group(g):\n",
        "    g = g.sort_values(\"createTimeISO\")\n",
        "    freq = len(g)\n",
        "\n",
        "    rows = []\n",
        "    for _, r in g.iterrows():\n",
        "        rows.append(\n",
        "            f\"user={r.get('anon_user', '')}, \"\n",
        "            f\"likes={r.get('likes', '')}, \"\n",
        "            f\"createdAt={r.get('createTimeISO', '')}\"\n",
        "        )\n",
        "\n",
        "    return pd.Series({\n",
        "        \"frequency\": freq,\n",
        "        \"instances\": \"\\n\".join(rows)\n",
        "    })\n",
        "\n",
        "\n",
        "with warnings.catch_warnings():\n",
        "    warnings.simplefilter(\"ignore\")\n",
        "    summary_tiktok = duplicates_full.groupby(text_col).apply(summarize_group).reset_index()\n",
        "\n",
        "\n",
        "summary_tiktok = summary_tiktok.sort_values(\"frequency\", ascending=False).reset_index(drop=True)\n",
        "\n",
        "\n",
        "summary_tiktok.index = summary_tiktok.index + 1\n",
        "\n",
        "# Display\n",
        "pd.set_option(\"display.max_colwidth\", None)\n",
        "\n",
        "display(\n",
        "    summary_tiktok.style.set_properties(\n",
        "        subset=[text_col],\n",
        "        **{\n",
        "            \"white-space\": \"normal\",\n",
        "            \"max-width\": \"300px\",\n",
        "        }\n",
        "    ).set_properties(\n",
        "        subset=[\"instances\"],\n",
        "        **{\n",
        "            \"white-space\": \"pre-wrap\",\n",
        "            \"max-width\": \"1000px\",\n",
        "            \"font-family\": \"monospace\"\n",
        "        }\n",
        "    )\n",
        ")\n"
      ],
      "metadata": {
        "id": "tfgbtPCwvX4Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2.0 Opinion leaders**"
      ],
      "metadata": {
        "id": "ILGi6CTG8XAM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The opinion leaders are identified using the benchmark of 75% of the total likecount/upvotes of the overall dataset. After knowing how many comments are in this subset, they are reviewed with their ranking, stable index, like count and text."
      ],
      "metadata": {
        "id": "IqC5WgFWrbtF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv(CLEAN_TIKTOK_DATA)\n",
        "\n",
        "LIKES_COL = \"likes\"\n",
        "\n",
        "if LIKES_COL not in df.columns:\n",
        "    raise ValueError(f\"Column '{LIKES_COL}' not found. Available columns: {list(df.columns)}\")\n",
        "\n",
        "df = df[df[LIKES_COL].notna() & (df[LIKES_COL] >= 0)]\n",
        "\n",
        "\n",
        "total_comments = len(df)\n",
        "total_likes = df[LIKES_COL].sum()\n",
        "\n",
        "\n",
        "if total_comments == 0 or total_likes == 0:\n",
        "    print(\"No comments or no likes in the dataset.\")\n",
        "else:\n",
        "    target_likes_75 = 0.75 * total_likes\n",
        "\n",
        "    df_sorted = df.sort_values(LIKES_COL, ascending=False).reset_index(drop=True)\n",
        "    df_sorted[\"cum_likes\"] = df_sorted[LIKES_COL].cumsum()\n",
        "\n",
        "    first_row_reaching_75 = df_sorted[df_sorted[\"cum_likes\"] >= target_likes_75].index.min()\n",
        "    comments_needed = int(first_row_reaching_75) + 1\n",
        "\n",
        "    percent_of_dataset = (comments_needed / total_comments) * 100\n",
        "\n",
        "    print(f\"Total comments: {total_comments:,}\")\n",
        "    print(f\"Total likes: {total_likes:,}\")\n",
        "    print(f\"75% benchmark of total likes: {target_likes_75:,.0f}\")\n",
        "    print(f\"Comments needed to reach 75% of likes: {comments_needed:,}\")\n",
        "    print(f\"These comments represent {percent_of_dataset:.2f}% of the dataset.\")\n"
      ],
      "metadata": {
        "id": "vOttSOQS46xg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv(CLEAN_TIKTOK_DATA)\n",
        "\n",
        "opinionleaders_tiktok = (\n",
        "    df.sort_values('likes', ascending=False)\n",
        "      .head(35)\n",
        "      .copy()\n",
        ")\n",
        "\n",
        "opinionleaders_tiktok[\"stable_index\"] = opinionleaders_tiktok.index\n",
        "\n",
        "opinionleaders_tiktok.index = range(1, len(opinionleaders_tiktok) + 1)\n",
        "\n",
        "\n",
        "opinionleaders_tiktok = opinionleaders_tiktok[[\"likes\", \"stable_index\", \"comment\"]]\n",
        "\n",
        "styled_table = opinionleaders_tiktok.style.set_properties(\n",
        "    subset=[\"comment\"],\n",
        "    **{\n",
        "        \"white-space\": \"normal\",\n",
        "        \"word-wrap\": \"break-word\",\n",
        "        \"width\": \"550px\"\n",
        "    }\n",
        ")\n",
        "\n",
        "styled_table\n"
      ],
      "metadata": {
        "id": "CcTugiEzwD3i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2.1 Thematic Analysis**"
      ],
      "metadata": {
        "id": "WmuiES0L4Ikw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This part thematically codes each comment within the subset of opinion leaders. The following categorizations have been identified:"
      ],
      "metadata": {
        "id": "ELKhoSA29rzw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-  **Humor**: Jokes that involve self-comparison (in reference to the models)\n",
        "-   **Overall Review**: Overall appreciation, with minimal context\n",
        "-   **Ideal Modelling Standard**: Discussion about the expectations and ideals of a model\n",
        "-   **Model Praise**: Naming or referring to specific models and appreciation for their performance/beauty/presence.\n",
        "\n",
        "\n",
        "***Artists***\n",
        "-   **Missy Elliot**: mixed sentiment\n",
        "-   **Twice**: Mostly negative feedback\n",
        "-   **Karol G**: Overwhelmingly positive\n",
        "-   **Madison Beer**: Generally praised\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "S82A3zOJ4Nop"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below follows the thematic distribution of comments and likecounts, visualised in a pie chart. Furthermore, all comments and their related themes are be provided as well."
      ],
      "metadata": {
        "id": "yTMMBYmQCLM3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from pathlib import Path\n",
        "\n",
        "# --- Load data ---\n",
        "df_clean = pd.read_csv(CLEAN_TIKTOK_DATA)\n",
        "print(f\"Loaded TikTok dataset with {len(df_clean):,} rows.\")\n",
        "\n",
        "if \"stable_index\" not in df_clean.columns:\n",
        "    df_clean[\"stable_index\"] = df_clean.index\n",
        "    print(\n",
        "        \"âš ï¸ 'stable_index' not found â€“ created from current index. \"\n",
        "        \"Make sure this matches how you chose indices for themes_by_index.\"\n",
        "    )\n",
        "\n",
        "themes_by_index = {\n",
        "    \"Humor\": [633, 637],\n",
        "    \"Model Praise\": [546, 274, 62, 55, 56, 53, 288],\n",
        "    \"Ideal Modelling Standard\": [57, 58, 179, 63, 64, 65, 59, 56, 53],\n",
        "    \"Missy Elliot\": [1661],\n",
        "    \"Twice\": [762, 763, 760, 774, 783, 766, 169, 779, 769],\n",
        "    \"Karol G\": [538, 767, 777, 542, 182, 761, 765],\n",
        "    \"Madison Beer\": [765, 771],\n",
        "    \"Overall Review\": [171],\n",
        "}\n",
        "\n",
        "text_col_name = None\n",
        "for col in [\"text\", \"content\", \"body\", \"comment\"]:\n",
        "    if col in df_clean.columns:\n",
        "        text_col_name = col\n",
        "        break\n",
        "if text_col_name is None:\n",
        "    raise ValueError(\"No text column found (expected 'text', 'content', 'body', or 'comment').\")\n",
        "\n",
        "likes_col_name = \"likes\"\n",
        "if likes_col_name not in df_clean.columns:\n",
        "    raise ValueError(f\"Likes column '{likes_col_name}' not found in df_clean.\")\n",
        "\n",
        "subset = df_clean[[\"stable_index\", text_col_name, likes_col_name]].copy()\n",
        "subset = subset.rename(columns={text_col_name: \"comment\", likes_col_name: \"likeCount\"})\n",
        "\n",
        "subset[\"likeCount\"] = subset[\"likeCount\"].fillna(0).astype(int)\n",
        "subset[\"comment\"] = subset[\"comment\"].astype(str).fillna(\"\").str.strip()\n",
        "\n",
        "top_comments_df = (\n",
        "    subset.sort_values(\"likeCount\", ascending=False)\n",
        "          .head(35)[[\"stable_index\", \"comment\", \"likeCount\"]]\n",
        "          .copy()\n",
        ")\n",
        "\n",
        "ranked = top_comments_df.reset_index(drop=True).copy()\n",
        "\n",
        "index_to_themes = {}\n",
        "for theme, indices in themes_by_index.items():\n",
        "    for idx in indices:\n",
        "        index_to_themes.setdefault(idx, []).append(theme)\n",
        "\n",
        "def join_labels(idx):\n",
        "    labs = index_to_themes.get(idx, [])\n",
        "    return \", \".join(labs) if labs else \"\"\n",
        "\n",
        "ranked[\"themes\"] = ranked[\"stable_index\"].apply(join_labels)\n",
        "\n",
        "missing = ranked.loc[ranked[\"themes\"].eq(\"\"), \"stable_index\"].tolist()\n",
        "if missing:\n",
        "    print(f\"âš ï¸ stable_index values in top-35 with NO theme label: {missing}\")\n",
        "else:\n",
        "    print(\"âœ… All top-35 TikTok comments have at least one theme label.\")\n",
        "\n",
        "ranked[\"theme_list\"] = ranked[\"themes\"].apply(\n",
        "    lambda x: x.split(\", \") if isinstance(x, str) and x else []\n",
        ")\n",
        "\n",
        "long = ranked.explode(\"theme_list\").rename(columns={\"theme_list\": \"theme\"})\n",
        "long = long[long[\"theme\"] != \"\"]\n",
        "\n",
        "counts = long.groupby(\"theme\")[\"stable_index\"].nunique().sort_values(ascending=False)\n",
        "likes = long.groupby(\"theme\")[\"likeCount\"].sum().sort_values(ascending=False)\n",
        "\n",
        "total_likes = likes.sum()\n",
        "\n",
        "summary_df = pd.DataFrame({\n",
        "    \"Comments\": counts,\n",
        "    \"Likes\": likes,\n",
        "    \"Share of Likes (%)\": (likes / total_likes * 100).round(1),\n",
        "}).sort_values(\"Likes\", ascending=False)\n",
        "\n",
        "print(\"\\nðŸ“Š THEME SUMMARY (Top 35 TikTok Comments)\\n\")\n",
        "print(summary_df.to_string())\n",
        "\n",
        "plt.figure(figsize=(10, 7))\n",
        "\n",
        "wedges, texts, autotexts = plt.pie(\n",
        "    summary_df[\"Likes\"],\n",
        "    labels=None,\n",
        "    autopct=\"%1.1f%%\",\n",
        "    startangle=140,\n",
        "    pctdistance=0.75\n",
        ")\n",
        "\n",
        "plt.title(\"Thematic Engagement Distribution\")\n",
        "\n",
        "plt.legend(\n",
        "    wedges,\n",
        "    summary_df.index,\n",
        "    title=\"Theme\",\n",
        "    loc=\"center left\",\n",
        "    bbox_to_anchor=(1.02, 0.5),\n",
        "    frameon=False\n",
        ")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# --- GitHub-ready output path ---\n",
        "OUTPUT_DIR = Path(\"data/processed\")\n",
        "if not OUTPUT_DIR.exists():\n",
        "    alt = Path(\"../data/processed\")\n",
        "    if alt.parent.exists():\n",
        "        OUTPUT_DIR = alt\n",
        "\n",
        "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "OUT_PATH = OUTPUT_DIR / \"top35_tiktok_comments_labeled_by_theme.csv\"\n",
        "ranked.to_csv(OUT_PATH, index=False, encoding=\"utf-8\")\n",
        "print(f\"\\nðŸ“ Saved labeled top-35 TikTok comments to: {OUT_PATH}\")\n"
      ],
      "metadata": {
        "id": "JzN8eyehC2YU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import textwrap\n",
        "from pathlib import Path\n",
        "\n",
        "\n",
        "CANDIDATE_PATHS = [\n",
        "    Path(\"data/processed/top35_tiktok_comments_labeled_by_theme.csv\"),\n",
        "    Path(\"../data/processed/top35_tiktok_comments_labeled_by_theme.csv\"),\n",
        "]\n",
        "\n",
        "LABELED_PATH = next((p for p in CANDIDATE_PATHS if p.exists()), None)\n",
        "if LABELED_PATH is None:\n",
        "    raise FileNotFoundError(\n",
        "        \"Could not find top35_tiktok_comments_labeled_by_theme.csv.\\n\"\n",
        "        \"Run the previous code block first to generate it.\\n\"\n",
        "        \"Expected: data/processed/top35_tiktok_comments_labeled_by_theme.csv\"\n",
        "    )\n",
        "\n",
        "df_labeled_comments = pd.read_csv(LABELED_PATH)\n",
        "print(f\"âœ… Loaded labeled TikTok comments from: {LABELED_PATH}\")\n",
        "\n",
        "if \"index\" in df_labeled_comments.columns:\n",
        "    df_labeled_comments = df_labeled_comments.rename(columns={\"index\": \"stable_index\"})\n",
        "\n",
        "df_labeled_comments[\"themes\"] = df_labeled_comments[\"themes\"].apply(\n",
        "    lambda x: [theme.strip() for theme in x.split(\",\")] if pd.notna(x) and x else []\n",
        ")\n",
        "\n",
        "all_unique_themes = sorted(\n",
        "    {theme for sublist in df_labeled_comments[\"themes\"] for theme in sublist}\n",
        ")\n",
        "\n",
        "MAX_WIDTH = 100\n",
        "\n",
        "print(\"\\n=== TikTok Comments by Theme (Top 35) ===\\n\")\n",
        "\n",
        "for theme in all_unique_themes:\n",
        "    print(f\"\\n--- ðŸ”µ Theme: {theme} ---\")\n",
        "\n",
        "    theme_comments = df_labeled_comments[\n",
        "        df_labeled_comments[\"themes\"].apply(lambda x: theme in x)\n",
        "    ]\n",
        "\n",
        "    if theme_comments.empty:\n",
        "        print(f\"  No comments found for '{theme}'.\")\n",
        "        continue\n",
        "\n",
        "    theme_comments = theme_comments.sort_values(by=\"likeCount\", ascending=False)\n",
        "\n",
        "    for i, row in enumerate(theme_comments.itertuples(index=False), start=1):\n",
        "        wrapped_comment = textwrap.fill(\n",
        "            row.comment,\n",
        "            width=MAX_WIDTH,\n",
        "            break_long_words=False,\n",
        "            replace_whitespace=False,\n",
        "        )\n",
        "        print(f\"  {i}. ðŸ‘ {row.likeCount} likes  |  Index: {row.stable_index}\")\n",
        "        print(f\"     {wrapped_comment}\")\n",
        "        print(\"-\" * MAX_WIDTH)\n"
      ],
      "metadata": {
        "id": "BNZzOfaWC-eF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3.0 Sentiment Analysis**"
      ],
      "metadata": {
        "id": "7RpzFQgD8jHV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This includes a sentiment analysis on the overall dataset, as well as a manual review of 100 comments. Here, the doubtful or incorrect classifications are separated for closer review, before calculating the accuracy rate.  "
      ],
      "metadata": {
        "id": "vOcPXCbxxU91"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q transformers torch\n",
        "\n",
        "import pandas as pd\n",
        "import torch\n",
        "import time\n",
        "from pathlib import Path\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline\n",
        "\n",
        "# INPUT\n",
        "df = pd.read_csv(CLEAN_TIKTOK_DATA)\n",
        "\n",
        "# OUTPUT (GitHub/Colab friendly)\n",
        "OUTPUT_DIR = Path(\"data/processed\")\n",
        "if not OUTPUT_DIR.exists():\n",
        "    alt = Path(\"../data/processed\")\n",
        "    if alt.parent.exists():\n",
        "        OUTPUT_DIR = alt\n",
        "\n",
        "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "SENTIMENT_OUTPUT_PATH = OUTPUT_DIR / \"cleaned_tiktok_data_sentiment.csv\"\n",
        "\n",
        "\n",
        "text_col = next((c for c in [\"comment\", \"text\", \"content\", \"body\"] if c in df.columns), None)\n",
        "if text_col is None:\n",
        "    raise ValueError(\"No text column found (expected one of: comment, text, content, body).\")\n",
        "\n",
        "df = df[~df[text_col].isin([\"[deleted]\", \"[removed]\"])].reset_index(drop=True)\n",
        "df[text_col] = df[text_col].fillna(\"\").astype(str)\n",
        "\n",
        "print(f\"âœ… Loaded {len(df):,} TikTok comments\")\n",
        "print(f\"ðŸ“ Using text column: {text_col}\")\n",
        "\n",
        "# Model\n",
        "model_name = \"cardiffnlp/twitter-xlm-roberta-base-sentiment\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "    model_name,\n",
        "    model_max_length=512,\n",
        "    truncation=True,\n",
        "    padding_side=\"right\"\n",
        ")\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
        "\n",
        "device = 0 if torch.cuda.is_available() else -1\n",
        "sentiment_pipeline = pipeline(\n",
        "    \"text-classification\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    device=device\n",
        ")\n",
        "\n",
        "\n",
        "texts = df[text_col].tolist()\n",
        "batch_size = 128\n",
        "results = []\n",
        "\n",
        "print(\"\\nðŸ”„ Running 3-class sentiment (pos/neu/neg) with truncation to 512 tokens...\\n\")\n",
        "for i in range(0, len(texts), batch_size):\n",
        "    batch = texts[i:i + batch_size]\n",
        "    out = sentiment_pipeline(batch, truncation=True, padding=True, max_length=512)\n",
        "    results.extend(out)\n",
        "\n",
        "    if (i // batch_size) % 10 == 0:\n",
        "        print(f\"Processed {min(i + batch_size, len(texts))}/{len(texts)}\", end=\"\\r\")\n",
        "\n",
        "    time.sleep(0.01)\n",
        "\n",
        "print(\"\\nâœ… Sentiment analysis complete!\")\n",
        "\n",
        "df[\"sentiment_label\"] = [r[\"label\"].upper() for r in results]\n",
        "df[\"sentiment_score\"] = [r[\"score\"] for r in results]\n",
        "\n",
        "\n",
        "df.to_csv(SENTIMENT_OUTPUT_PATH, index=False, encoding=\"utf-8\")\n",
        "print(f\"ðŸ’¾ Saved TikTok sentiment file to:\\n{SENTIMENT_OUTPUT_PATH}\")\n",
        "\n",
        "df[[text_col, \"sentiment_label\", \"sentiment_score\"]].head(5)\n"
      ],
      "metadata": {
        "id": "C02_fSZn69Ga"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "\n",
        "CANDIDATE_PATHS = [\n",
        "    Path(\"data/processed/cleaned_tiktok_data_sentiment.csv\"),\n",
        "    Path(\"../data/processed/cleaned_tiktok_data_sentiment.csv\"),\n",
        "]\n",
        "\n",
        "TIKTOK_SENTIMENT = next((p for p in CANDIDATE_PATHS if p.exists()), None)\n",
        "if TIKTOK_SENTIMENT is None:\n",
        "    raise FileNotFoundError(\n",
        "        \"Could not find cleaned_tiktok_data_sentiment.csv.\\n\"\n",
        "        \"Run the sentiment generation block first.\\n\"\n",
        "        \"Expected: data/processed/cleaned_tiktok_data_sentiment.csv\"\n",
        "    )\n",
        "\n",
        "df_tiktok = pd.read_csv(TIKTOK_SENTIMENT)\n",
        "print(f\"Loaded {len(df_tiktok):,} TikTok rows from: {TIKTOK_SENTIMENT}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "I5M2sc6RSX3z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df_loaded = pd.read_csv(TIKTOK_SENTIMENT)\n",
        "\n",
        "text_col = next(\n",
        "    (c for c in [\"text\", \"content\", \"body\", \"comment\"] if c in df_loaded.columns),\n",
        "    None\n",
        ")\n",
        "\n",
        "pd.set_option(\"display.max_rows\", 100)\n",
        "pd.set_option(\"display.max_colwidth\", None)\n",
        "\n",
        "df_loaded[[text_col, \"sentiment_label\", \"sentiment_score\"]].head(100)\n"
      ],
      "metadata": {
        "id": "MOA1K8utILvI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv(TIKTOK_SENTIMENT)\n",
        "\n",
        "text_col = next((c for c in [\"comment\",\"text\",\"content\",\"body\"] if c in df.columns), None)\n",
        "if text_col is None:\n",
        "    raise ValueError(\"No text column found in dataset.\")\n",
        "\n",
        "indices = [1, 6, 8, 11, 15, 16, 21, 26, 32, 51, 54, 69, 70, 71, 75, 81, 86, 89, 90, 97]\n",
        "\n",
        "df.loc[indices, [text_col, \"sentiment_label\", \"sentiment_score\"]]\n"
      ],
      "metadata": {
        "id": "Hc9cVXfKygNl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Review of misclassifications**\n",
        "\n",
        "\n",
        "This dataset includes a lot of slang, like â€˜ ateâ€™, â€˜eat downâ€™ and â€˜ ate hardâ€™, which is a complimentary reference yet wrongly classified as negative. Comments where people expressed their aspirations for the show, like â€˜only who can WALKâ€™ being desired, not â€˜an average guy playing next to LeBronâ€™, subtly highlight the inadequacies of the show, therefore better suited as a negative sentiment, although the current neutral sentiment is not completely wrong either. Furthermore, many comments included mixed emotions, expressing a positive review yet wondering why there are so many negative responses. Technically this would be a positive sentiment, yet it is classified as negative. Then there are some comments with little to no context, naming the models with some emojiâ€™s, or including expressions like â€˜my jaw was on the floorâ€™, or â€˜i screamedâ€™ which could be interpreted in different ways. This makes categorisation difficult, as more contextual awareness is needed. Therefore, classifications of positive or negative seem unreliable.\n",
        "\n"
      ],
      "metadata": {
        "id": "QsOOJr5cPilT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Accuracy rate of 80%*"
      ],
      "metadata": {
        "id": "-OssHpwyzJlM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3.1 Sentiment Popularity**"
      ],
      "metadata": {
        "id": "nSQ4HKhYmq2I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This reviews the popularity of each sentiment, first by reviewing their overall presence in the dataset, the top comments of each sentiment and the average like count per category."
      ],
      "metadata": {
        "id": "OybkOG1YxrZZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "df = pd.read_csv(TIKTOK_SENTIMENT)\n",
        "print(f\"âœ… Loaded {len(df):,} rows with sentiment data\")\n",
        "\n",
        "\n",
        "counts = df[\"sentiment_label\"].value_counts()\n",
        "percentages = (counts / counts.sum() * 100).round(2)\n",
        "\n",
        "plt.figure(figsize=(6, 4))\n",
        "bars = plt.bar(counts.index, counts.values, color=[\"#E74C3C\", \"#2ECC71\", \"#F1C40F\"])\n",
        "\n",
        "plt.title(\"Tik Tok Sentiment Distribution\", fontsize=13, pad=10)\n",
        "plt.xlabel(\"Sentiment Category\", fontsize=11)\n",
        "plt.ylabel(\"Number of Comments\", fontsize=11)\n",
        "plt.xticks(rotation=0)\n",
        "\n",
        "\n",
        "for bar, pct in zip(bars, percentages):\n",
        "    plt.text(\n",
        "        bar.get_x() + bar.get_width() / 2,\n",
        "        bar.get_height() + counts.max() * 0.01,\n",
        "        f\"{pct:.1f}%\",\n",
        "        ha=\"center\",\n",
        "        va=\"bottom\",\n",
        "        fontsize=10,\n",
        "\n",
        "    )\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nðŸ“Š Sentiment Breakdown:\")\n",
        "for label, pct in percentages.items():\n",
        "    print(f\"{label}: {pct:.2f}% ({counts[label]} comments)\")\n"
      ],
      "metadata": {
        "id": "GGxYitTW93IE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "\n",
        "df_sentiment = pd.read_csv(TIKTOK_SENTIMENT)\n",
        "df_sentiment.columns = df_sentiment.columns.str.lower()\n",
        "\n",
        "\n",
        "text_col = next((c for c in [\"comment\",\"text\",\"content\",\"body\"] if c in df_sentiment.columns), None)\n",
        "if text_col is None:\n",
        "    raise ValueError(\"No comment column found.\")\n",
        "\n",
        "df_sentiment = df_sentiment.rename(columns={\n",
        "    text_col: \"comment\",\n",
        "    \"diggcount\": \"likes\"\n",
        "})\n",
        "\n",
        "\n",
        "df_sentiment[\"likes\"] = pd.to_numeric(df_sentiment[\"likes\"], errors=\"coerce\").fillna(0).astype(int)\n",
        "\n",
        "\n",
        "df_sentiment[\"sentiment_label\"] = df_sentiment[\"sentiment_label\"].astype(str).str.lower()\n",
        "\n",
        "\n",
        "if \"sentiment_score\" in df_sentiment.columns:\n",
        "    df_sentiment = df_sentiment.rename(columns={\"sentiment_score\": \"confidence\"})\n",
        "\n",
        "\n",
        "def print_top(df, sentiment_type, icon):\n",
        "    subset = (\n",
        "        df[df[\"sentiment_label\"] == sentiment_type]\n",
        "        .sort_values(\"likes\", ascending=False)\n",
        "        .head(15)\n",
        "    )\n",
        "\n",
        "    print(f\"\\n{icon} Top 15 Most-Liked {sentiment_type.capitalize()} Comments:\\n\")\n",
        "    for _, row in subset.iterrows():\n",
        "        snippet = str(row[\"comment\"])[:500]\n",
        "        print(f\"ðŸ’¬ {snippet}\")\n",
        "        print(f\"   ðŸ”¹ Sentiment: {row['sentiment_label']} \"\n",
        "              f\"({row['confidence']:.3f}) | Likes: {row['likes']}\\n\")\n",
        "\n",
        "print_top(df_sentiment, \"negative\", \"ðŸ”´\")\n",
        "print_top(df_sentiment, \"positive\", \"ðŸŸ¢\")\n",
        "print_top(df_sentiment, \"neutral\", \"ðŸŸ¡\")\n"
      ],
      "metadata": {
        "id": "CupkQyEL_nMX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "df = pd.read_csv(TIKTOK_SENTIMENT)\n",
        "\n",
        "df.columns = df.columns.str.lower()\n",
        "\n",
        "\n",
        "text_col = next((c for c in [\"comment\",\"text\",\"content\",\"body\"] if c in df.columns), None)\n",
        "if text_col is None:\n",
        "    raise ValueError(\"No comment column found.\")\n",
        "\n",
        "\n",
        "df = df.rename(columns={\n",
        "    text_col: \"comment\",\n",
        "    \"diggcount\": \"likes\"\n",
        "})\n",
        "\n",
        "df[\"likes\"] = pd.to_numeric(df[\"likes\"], errors=\"coerce\").fillna(0)\n",
        "\n",
        "df[\"sentiment_label\"] = df[\"sentiment_label\"].astype(str).str.lower()\n",
        "\n",
        "\n",
        "df[\"sentiment_label_plot\"] = df[\"sentiment_label\"].str.title()\n",
        "\n",
        "sentiment_stats = (\n",
        "    df.groupby(\"sentiment_label_plot\")[\"likes\"]\n",
        "      .agg([\"mean\", \"sum\", \"count\"])\n",
        "      .reset_index()\n",
        "      .sort_values(\"mean\", ascending=False)\n",
        ")\n",
        "\n",
        "# Colors for chart\n",
        "color_map = {\n",
        "    \"Positive\": \"#2ECC71\",\n",
        "    \"Neutral\": \"#F1C40F\",\n",
        "    \"Negative\": \"#E74C3C\"\n",
        "\n",
        "}\n",
        "colors = [color_map.get(label, \"gray\") for label in sentiment_stats[\"sentiment_label_plot\"]]\n",
        "\n",
        "plt.figure(figsize=(7,4))\n",
        "bars = plt.bar(sentiment_stats[\"sentiment_label_plot\"], sentiment_stats[\"mean\"], color=colors)\n",
        "plt.title(\"Average Likes per Sentiment Category (Tik Tok)\")\n",
        "plt.ylabel(\"Average Likes per Comment\")\n",
        "plt.xlabel(\"Sentiment\")\n",
        "\n",
        "y_pad = sentiment_stats[\"mean\"].max() * 0.01\n",
        "for bar in bars:\n",
        "    plt.text(\n",
        "        bar.get_x() + bar.get_width()/2,\n",
        "        bar.get_height() + y_pad,\n",
        "        f\"{bar.get_height():.0f}\",\n",
        "        ha=\"center\",\n",
        "        va=\"bottom\",\n",
        "        fontsize=10\n",
        "    )\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nðŸ“Š Sentiment Popularity Summary:\")\n",
        "for _, row in sentiment_stats.iterrows():\n",
        "    print(f\"{row['sentiment_label_plot']}: {int(row['count'])} comments | \"\n",
        "          f\"Avg Likes: {row['mean']:.1f} | Total Likes: {row['sum']:.0f}\")\n"
      ],
      "metadata": {
        "id": "D4VL389F_4Uv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4.0 Text Frequency Analysis**"
      ],
      "metadata": {
        "id": "8POCqoPa9Gzb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This part includes an analysis of the most frequent words in the dataset. From the most frequent or thematically relevant words, the top 10 most liked comments are reviewed, to gain deeper contextual awareness."
      ],
      "metadata": {
        "id": "G9OH_AdfcRf9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import nltk\n",
        "import pandas as pd\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from IPython.display import display\n",
        "\n",
        "nltk.download(\"stopwords\", quiet=True)\n",
        "\n",
        "df = pd.read_csv(CLEAN_TIKTOK_DATA)\n",
        "print(f\"âœ… Loaded {len(df):,} rows from {CLEAN_TIKTOK_DATA}\")\n",
        "\n",
        "text_col = next((c for c in [\"comment\", \"text\", \"content\", \"body\"] if c in df.columns), None)\n",
        "if text_col is None:\n",
        "    raise ValueError(\"âŒ No text column found (expected one of: comment, text, content, body).\")\n",
        "\n",
        "df[text_col] = df[text_col].fillna(\"\").astype(str).str.strip()\n",
        "df = df[df[text_col].str.len() >= 5].copy()\n",
        "print(f\"ðŸ§¹ Kept {len(df):,} comments with â‰¥5 characters\")\n",
        "\n",
        "stop_words = set(stopwords.words(\"english\"))\n",
        "\n",
        "project_stopwords = {\"dont\", \"thats\", \"didnt\", \"like\", \"think\", \"would\"}\n",
        "stop_words |= project_stopwords\n",
        "\n",
        "junk_tokens = {\"s\", \"t\", \"m\", \"g\", \"1\", \"000\", \"la\", \"de\", \"se\", \"las\", \"el\", \"que\"}\n",
        "stop_words |= junk_tokens\n",
        "\n",
        "\n",
        "vectorizer = CountVectorizer(\n",
        "    stop_words=list(stop_words),\n",
        "    lowercase=True,\n",
        "    token_pattern=r\"(?u)\\b[a-zA-Z]{2,}\\b\"\n",
        ")\n",
        "\n",
        "X = vectorizer.fit_transform(df[text_col])\n",
        "counts = X.sum(axis=0).A1\n",
        "words = vectorizer.get_feature_names_out()\n",
        "\n",
        "top_words = (\n",
        "    pd.DataFrame({\"word\": words, \"count\": counts})\n",
        "      .sort_values(\"count\", ascending=False)\n",
        "      .head(15)\n",
        "      .reset_index(drop=True)\n",
        ")\n",
        "\n",
        "top_words.index = top_words.index + 1\n",
        "\n",
        "print(\"\\nTop 15 Words in TikTok Dataset:\")\n",
        "display(\n",
        "    top_words.style.set_properties(**{\n",
        "        \"border\": \"1px solid black\",\n",
        "        \"text-align\": \"left\",\n",
        "    })\n",
        ")\n"
      ],
      "metadata": {
        "id": "z0YeMfJOBNFg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pd.set_option(\"display.max_colwidth\", None)\n",
        "pd.set_option(\"display.max_rows\", 100)\n",
        "\n",
        "TOP_N_WORDS = 15\n",
        "TOP_N_COMMENTS = 15\n",
        "\n",
        "for word in top_words[\"word\"].head(TOP_N_WORDS):\n",
        "    subset = df[\n",
        "        df[text_col].str.contains(rf\"\\b{word}\\b\", case=False, regex=True)\n",
        "    ]\n",
        "\n",
        "    if subset.empty:\n",
        "        continue\n",
        "\n",
        "    top_comments = (\n",
        "        subset.sort_values(\"likes\", ascending=False)\n",
        "              .head(TOP_N_COMMENTS)[[text_col, \"likes\", \"stable_index\"]]\n",
        "              .reset_index(drop=True)\n",
        "    )\n",
        "\n",
        "    print(f\"\\nðŸ”¹ Top {TOP_N_COMMENTS} most-liked comments containing '{word}':\\n\")\n",
        "    display(top_comments)\n"
      ],
      "metadata": {
        "id": "VMZp22jVQu7T"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}